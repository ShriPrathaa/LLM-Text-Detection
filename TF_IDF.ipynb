{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 61542,
          "databundleVersionId": 6888007,
          "sourceType": "competition"
        },
        {
          "sourceId": 6977472,
          "sourceType": "datasetVersion",
          "datasetId": 4005256
        }
      ],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'llm-detect-ai-generated-text:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F61542%2F7516023%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240517%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240517T173929Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D1306dc58703c6fec9dea68315aa71946507add5312ac841d685d3b09548b4209bb1b3c06dcb63702621900f1dbb30582be548ee19a472cb51bb66e719ccdcc4960aa906b9db0b309138c5707a3835133beacd062f68895c4ee38ed293101e1da35ca0561289da5dcbe18bb61c0a01a9376f00f4f3cd37bfdaee931625db9e0d22980f8e803bc11d083df9d68fd0de99ec6e35340f46a0a82fbfdf54f1e66a807a0cb09847c232fe3eafe9a76729bddb0dc41629448679174c13738d083ab6c9b7364678c119a1a106402d77c974aaee1557999feab82d7697eb4c747c9dfa330d5dde74cfb537fe8063772258a523334ae85edb3003403d0c56a89d3fbbce3ee,daigt-v2-train-dataset:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4005256%2F6977472%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240517%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240517T173929Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5048808bcc7cecdea1b083637a8f029fe43c482695bb91a1e1e848e11287ff25d4a090386d24074a520aaf6eb698f60d56a977f9d18103565a2f72735eddbe8f1ac01605b5ba331b150c0fc26745d243bad017ad75ab4edbc9dde1e7cdb5c0f679d6317cb38529bbf20ddac4b3d9c6ecd70f53aa9b1c3742859167bd9c9b37d74d9dcfe6be12783f590e4681dcda058e48ad917e8d5af8288852ca376ca0849453f2188608f27f62a6622d1a034877a7b30016a113fbd70334b9f5eadede967aaefdc256978f4af73e32cb5d7ddb551a2347e444f0832c94e179bdcdf2e02988e8863678fe28af35b8d50e6fc7afad65b01fbd5c17af89c0529a0f12e225bddb'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_3zr9rMyoz4x"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import gc\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import numpy as np\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from tokenizers import (\n",
        "    decoders,\n",
        "    models,\n",
        "    normalizers,\n",
        "    pre_tokenizers,\n",
        "    processors,\n",
        "    trainers,\n",
        "    Tokenizer,\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datasets import Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import VotingClassifier"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "execution": {
          "iopub.status.busy": "2024-01-17T05:53:14.811389Z",
          "iopub.execute_input": "2024-01-17T05:53:14.812353Z",
          "iopub.status.idle": "2024-01-17T05:53:24.585897Z",
          "shell.execute_reply.started": "2024-01-17T05:53:14.812309Z",
          "shell.execute_reply": "2024-01-17T05:53:24.584738Z"
        },
        "trusted": true,
        "id": "W1xWD1b_oz43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\n",
        "sub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n",
        "train = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:53:24.587737Z",
          "iopub.execute_input": "2024-01-17T05:53:24.588477Z",
          "iopub.status.idle": "2024-01-17T05:53:26.996959Z",
          "shell.execute_reply.started": "2024-01-17T05:53:24.588437Z",
          "shell.execute_reply": "2024-01-17T05:53:26.995795Z"
        },
        "trusted": true,
        "id": "UUlWkbqvoz44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "excluded_prompt_name_list = ['Distance learning','Grades for extracurricular activities','Summer projects']\n",
        "train = train[~(train['prompt_name'].isin(excluded_prompt_name_list))]\n",
        "train = train.drop_duplicates(subset=['text'])\n",
        "train.reset_index(drop=True, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:53:26.998292Z",
          "iopub.execute_input": "2024-01-17T05:53:26.998621Z",
          "iopub.status.idle": "2024-01-17T05:53:27.084678Z",
          "shell.execute_reply.started": "2024-01-17T05:53:26.998592Z",
          "shell.execute_reply": "2024-01-17T05:53:27.083528Z"
        },
        "trusted": true,
        "id": "UDCF2foaoz45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LOWERCASE = False\n",
        "VOCAB_SIZE = 30522"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:53:27.088542Z",
          "iopub.execute_input": "2024-01-17T05:53:27.089334Z",
          "iopub.status.idle": "2024-01-17T05:53:27.094297Z",
          "shell.execute_reply.started": "2024-01-17T05:53:27.089286Z",
          "shell.execute_reply": "2024-01-17T05:53:27.093113Z"
        },
        "trusted": true,
        "id": "uKOpee4Ooz46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Byte-Pair Encoding tokenizer\n",
        "raw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\"))\n",
        "raw_tokenizer.normalizer = normalizers.Sequence([normalizers.NFC()] + [normalizers.Lowercase()] if LOWERCASE else [])\n",
        "raw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n",
        "special_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
        "trainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens)\n",
        "dataset = Dataset.from_pandas(test[['text']])\n",
        "def train_corp_iter():\n",
        "    for i in range(0, len(dataset), 1000):\n",
        "        yield dataset[i : i + 1000][\"text\"]\n",
        "raw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=raw_tokenizer,\n",
        "    unk_token=\"[UNK]\",\n",
        "    pad_token=\"[PAD]\",\n",
        "    cls_token=\"[CLS]\",\n",
        "    sep_token=\"[SEP]\",\n",
        "    mask_token=\"[MASK]\",\n",
        ")\n",
        "tokenized_texts_test = []\n",
        "\n",
        "for text in tqdm(test['text'].tolist()):\n",
        "    tokenized_texts_test.append(tokenizer.tokenize(text))\n",
        "\n",
        "tokenized_texts_train = []\n",
        "\n",
        "for text in tqdm(train['text'].tolist()):\n",
        "    tokenized_texts_train.append(tokenizer.tokenize(text))\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:53:27.09603Z",
          "iopub.execute_input": "2024-01-17T05:53:27.096758Z",
          "iopub.status.idle": "2024-01-17T05:55:19.833756Z",
          "shell.execute_reply.started": "2024-01-17T05:53:27.096716Z",
          "shell.execute_reply": "2024-01-17T05:55:19.832888Z"
        },
        "trusted": true,
        "id": "EXmJZOPOoz46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dummy(text):\n",
        "    return text\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, analyzer = 'word',\n",
        "    tokenizer = dummy,\n",
        "    preprocessor = dummy,\n",
        "    token_pattern = None, strip_accents='unicode')\n",
        "\n",
        "vectorizer.fit(tokenized_texts_test)\n",
        "\n",
        "# Getting vocab\n",
        "vocab = vectorizer.vocabulary_\n",
        "\n",
        "print(vocab)\n",
        "\n",
        "vectorizer = TfidfVectorizer(ngram_range=(3, 5), lowercase=False, sublinear_tf=True, vocabulary=vocab,\n",
        "                            analyzer = 'word',\n",
        "                            tokenizer = dummy,\n",
        "                            preprocessor = dummy,\n",
        "                            token_pattern = None, strip_accents='unicode'\n",
        "                            )\n",
        "\n",
        "tf_train = vectorizer.fit_transform(tokenized_texts_train)\n",
        "tf_test = vectorizer.transform(tokenized_texts_test)\n",
        "\n",
        "del vectorizer\n",
        "gc.collect()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:55:19.835089Z",
          "iopub.execute_input": "2024-01-17T05:55:19.836037Z",
          "iopub.status.idle": "2024-01-17T05:58:25.069533Z",
          "shell.execute_reply.started": "2024-01-17T05:55:19.836Z",
          "shell.execute_reply": "2024-01-17T05:58:25.068359Z"
        },
        "trusted": true,
        "id": "-M4UFYNloz47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train['label'].values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:58:25.070881Z",
          "iopub.execute_input": "2024-01-17T05:58:25.072517Z",
          "iopub.status.idle": "2024-01-17T05:58:25.078255Z",
          "shell.execute_reply.started": "2024-01-17T05:58:25.07246Z",
          "shell.execute_reply": "2024-01-17T05:58:25.077092Z"
        },
        "trusted": true,
        "id": "Cj5T5Rkioz48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if len(test.text.values) <= 5:\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "else:\n",
        "    clf = MultinomialNB(alpha=0.02)\n",
        "#     clf2 = MultinomialNB(alpha=0.01)\n",
        "    sgd_model = SGDClassifier(max_iter=8000, tol=1e-4, loss=\"modified_huber\")\n",
        "    p6={'n_iter': 3000,\n",
        "        'verbose': -1,\n",
        "        'objective': 'cross_entropy',\n",
        "        'metric': 'auc',\n",
        "        'learning_rate': 0.00581909898961407,\n",
        "        'colsample_bytree': 0.78,\n",
        "        'colsample_bynode': 0.8,\n",
        "        'lambda_l1': 4.562963348932286,\n",
        "        'lambda_l2': 2.97485,\n",
        "        'min_data_in_leaf': 115,\n",
        "        'max_depth': 23,\n",
        "        'max_bin': 898}\n",
        "\n",
        "    lgb=LGBMClassifier(**p6)\n",
        "    cat=CatBoostClassifier(iterations=3000,\n",
        "                           verbose=0,\n",
        "                           l2_leaf_reg=6.6591278779517808,\n",
        "                           learning_rate=0.005599066836106983,\n",
        "                           subsample = 0.4,\n",
        "                           allow_const_label=True,\n",
        "                           loss_function = 'CrossEntropy')\n",
        "\n",
        "    weights = [0.07,0.31,0.31,0.31]\n",
        "\n",
        "    ensemble = VotingClassifier(estimators=[('mnb',clf),\n",
        "                                            ('sgd', sgd_model),\n",
        "                                            ('lgb',lgb),\n",
        "                                            ('cat', cat)\n",
        "                                           ],\n",
        "                                weights=weights, voting='soft')\n",
        "    ensemble.fit(tf_train, y_train)\n",
        "    gc.collect()\n",
        "    final_preds = ensemble.predict_proba(tf_test)[:,1]\n",
        "    sub['generated'] = final_preds\n",
        "    sub.to_csv('submission.csv', index=False)\n",
        "    sub"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-01-17T05:58:25.079686Z",
          "iopub.execute_input": "2024-01-17T05:58:25.080161Z",
          "iopub.status.idle": "2024-01-17T05:58:25.094635Z",
          "shell.execute_reply.started": "2024-01-17T05:58:25.080122Z",
          "shell.execute_reply": "2024-01-17T05:58:25.093676Z"
        },
        "trusted": true,
        "id": "7mP3QBoVoz48"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XIZ2q2tCoz49"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}